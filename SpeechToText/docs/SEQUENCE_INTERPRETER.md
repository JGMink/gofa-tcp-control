# Sequence Interpreter — Design Document

**File:** `SpeechToText/learning/sequence_interpreter.py`
**Model:** `claude-3-haiku-20240307`
**Last updated:** February 2026

---

## Overview

The sequence interpreter is the LLM layer of the GoFa robot arm voice control pipeline. It sits between raw speech text and executable robot instructions. Its job is to convert a natural language voice command into a structured sequence of named composites that the instruction compiler can execute.

It is a **three-pass LLM pipeline** — generation, validation, and conditional regeneration — with separate handling for creative commands, recovery commands, unknown items, and secondary/learning commands.

---

## Position in the Full Pipeline

```
Voice
  └─▶ Azure Speech-to-Text
        └─▶ Phrase bank (fuzzy match — fast path for known phrases)
              └─▶ Sequence Interpreter  ◀── this document
                    └─▶ Instruction Compiler  (composites → primitives)
                          └─▶ Executor  (primitives → tcp_commands.json)
                                └─▶ Unity / RobotStudio / ABB GoFa

                    └─▶ Memory Writer  (secondary commands only)
                          └─▶ instruction_set.json  (learned_composites)
                          └─▶ phrase_bank.json  (learned_aliases)
```

The sequence interpreter is only reached if the phrase bank fuzzy match does not return a confident hit. It handles everything the phrase bank can't: novel recipes, creative commands, modifiers, multi-zone builds, recovery, and learning directives.

---

## Instruction Tiers (Von Neumann ISA)

The interpreter operates within a three-tier instruction hierarchy defined in `instruction_set.json`:

| Tier | Name | Visibility | Description |
|------|------|-----------|-------------|
| 0 | **Primitives** | LLM-invisible | Atomic hardware calls: `move_to`, `move_relative`, `gripper_set`, `wait`, `set_speed`. The LLM never calls these directly. |
| 1 | **Composites** | LLM-visible | Stored programs built from primitives: `pick_up`, `add_layer`, `go_home`, `set_active_zone`, etc. The LLM generates sequences of these. |
| 2 | **Learned composites** | Names-only in context | Composites generated by the LLM and written to `instruction_set.json` at runtime. The LLM sees their names but not their sequences. |

The LLM is only ever allowed to call composites. The instruction compiler expands each composite into its primitive sequence before execution.

---

## Three-Pass Pipeline

### Pass 1 — Generation

**Purpose:** Convert voice command → JSON sequence
**Temperature:** `0` for standard commands, `1.0` for creative commands
**Max tokens:** `900`

**Input to LLM:**
- Full LLM context from `get_llm_context()`:
  - Available composite instructions with descriptions and parameters
  - Current scene state: items, locations, assembly zone stacks, active zone
  - Speed profiles and distance aliases from `motion_params`
  - Learned composite names (names only, no sequences)
- Structured prompt sections: OUTPUT FORMAT, RULES, CREATIVE COMMANDS, RECOVERY COMMANDS, SPEED+ACTION COMBINED, UNKNOWN/IMPOSSIBLE COMMANDS, SECONDARY/LEARNING COMMANDS, EXAMPLES
- If creative: the full CREATIVE MODE — AXES section with the pre-selected axis combination (see below)
- If Pass 3 retry: a CORRECTION REQUIRED section listing the specific issues to fix

**Output from LLM:**
```json
{
  "interpretation": "one sentence describing what the command means",
  "sequence": [
    {"instruction": "composite_name", "params": {"key": "value"}},
    ...
  ],
  "composite_name": "snake_case_name_or_null",
  "confidence": 0.95,
  "user_feedback": null,
  "creative_reasoning": null
}
```

**What happens if parse fails:** the raw text is preserved in `raw_response`, the result returns with `confidence: 0.0` and an empty sequence. Nothing is silently dropped.

---

### Pass 2 — Validation

**Purpose:** Check Pass 1 output for invalid instruction names, bad params, wrong zone usage
**Temperature:** `0` (always deterministic)
**Max tokens:** `700`
**Runs only if:** Pass 1 produced a non-empty sequence

**Input to LLM:**
- The voice command and interpretation from Pass 1
- The generated sequence from Pass 1
- Three explicit valid-name lists derived at runtime:
  - `valid_composites` — from `instruction_compiler.get_composites().keys()`
  - `valid_items` — from `instruction_compiler.get_items().keys()`
  - `valid_locations` — from `instruction_compiler.get_locations().keys()`

**Validation rules enforced:**
1. Every `instruction` value must be in `valid_composites` — fix or remove if not (e.g. `move_absolute` → remove)
2. Every `item` param must be in `valid_items` — remove steps with unknown items
3. Every `location` param must be in `valid_locations`
4. `move_relative` direction must be one of: `right`, `left`, `up`, `down`, `forward`, `backward`
5. Assembly sequences must use `add_layer`, not `transfer` or `place_at`
6. `set_active_zone` zone must be one of: `assembly_fixture`, `assembly_left`, `assembly_right`

**Output from LLM:**
```json
{
  "valid": true,
  "issues": ["list of what was wrong — empty if clean"],
  "sequence": [corrected sequence]
}
```

**Result fields set after Pass 2:**
- `sequence` → replaced with corrected version
- `validated` → `true` if no issues, `false` if issues found or validator response unparseable
- `validation_issues` → list of issue strings, shown in the app bubble
- `pass1_sequence` → snapshot of the original Pass 1 sequence (preserved for side-by-side display)

---

### Pass 3 — Regeneration

**Purpose:** If Pass 2 found real structural issues, feed them back to the generator for one correction retry
**Temperature:** `0` (correction passes are always deterministic)
**Max tokens:** `900`
**Runs only if:** Pass 2 found issues that are not "unparseable" or "API error"

**What gets injected into the prompt:**
A `CORRECTION REQUIRED (Pass 3)` section is prepended to the normal prompt:
```
━━━ CORRECTION REQUIRED (Pass 3) ━━━
Your previous response had these problems that the validator caught:
  - move_absolute is not a valid instruction
  - transfer used for assembly zone — should be add_layer

Please regenerate a corrected sequence that fixes ALL of these issues.
Use ONLY valid instruction names from the list above.
```

**Result:** if Pass 3 produces a valid response, its sequence replaces the Pass 2 output. Validation issues are tagged `[P3 fixed]`. If Pass 3 fails, the Pass 2 result is kept — Pass 3 is non-fatal.

---

## Creative Command Handling

### Detection

Creative commands are detected in Python **before** any LLM call, using a regex pattern list:

```python
_CREATIVE_PATTERNS = [
    r"\bimpress\b", r"\bgo wild\b", r"\bsurprise\b", r"\bcreative\b",
    r"\bbeautiful\b", r"\bwork of art\b", r"\bbuild a tower\b",
    r"\bbest .* can\b", r"\bsomething delicious\b", r"\bdo something\b",
    r"\bgo crazy\b", r"\bhave fun\b", r"\bfancy\b", r"\belaborate\b",
    ...
]
```

If `_is_creative(command)` returns `True`:
- Pass 1 temperature is set to `1.0` (probabilistic sampling, different output every run)
- `composite_name` is force-nulled in Python regardless of what the LLM returns (creative outputs are never saved as composites)
- The CREATIVE MODE — AXES section is injected into the prompt

### Axis System

The creative section uses a **two-axis composition model** instead of named archetypes. Python pre-selects one option from each axis using `random.choice()` and injects it as a concrete assignment before the LLM sees the prompt. The LLM cannot pick its own favorite — it receives a specific combination and must execute it.

**Axis 1 — Spatial Structure** (how layers are distributed across zones and height):

| Option | Instruction |
|--------|------------|
| `single_tall` | One zone only — stack as many layers as possible |
| `single_short` | One zone only — exactly 2–3 layers, stop there |
| `three_zone_split` | Use all three zones via `set_active_zone()` — three builds in sequence |
| `two_zone_contrast` | Two zones with opposing ingredient sets |
| `palindrome` | One zone — sequence reads identically forwards and backwards |
| `inverted` | One zone — reverse of conventional order, bread absolutely last |

**Axis 2 — Ingredient Logic** (which ingredients are used and why):

| Option | Instruction |
|--------|------------|
| `all_one` | One ingredient repeated 4–6 times, nothing else |
| `category_grouped` | All proteins first, then all vegetables, then starch/dairy |
| `full_set` | Every available ingredient at least once, unusual order |
| `random_ordered` | All ingredients in a non-obvious, non-conventional order |
| `doubled` | Exactly two ingredients alternating: A,B,A,B,A,B |
| `constrained` | Only ingredients explicitly named in the command (auto-selected when command names items) |

**Compatibility guards:** some axis combinations are logically contradictory and blocked:

```python
_INCOMPATIBLE_PAIRS = {
    "palindrome": {"random_ordered", "category_grouped", "full_set"},
    "inverted":   {"all_one"},
}
```

If the random draw produces an incompatible pair, Python retries up to 10 times until a valid pair is found.

**Constrained override:** if the voice command names specific ingredients (e.g. "do something with the lettuce and tomato"), Axis 2 is automatically set to `constrained` with those ingredients listed explicitly. Axis 1 is still random.

**What gets injected into the prompt for creative commands:**
```
YOUR ASSIGNED COMBINATION FOR THIS COMMAND:

  AXIS 1 (spatial structure): palindrome
  → one zone — the ingredient sequence must read IDENTICALLY forwards and backwards...

  AXIS 2 (ingredient logic): doubled
  → pick exactly TWO ingredients and alternate them: A,B,A,B,A,B. Nothing else.

You MUST follow both axes literally when building the sequence.
...
- palindrome: write out your planned sequence first in creative_reasoning, verify it
  reads the same backwards, THEN generate the JSON steps.
- inverted: the LAST step before go_home() must be add_layer(bread). Bread goes last.
```

**Result fields for creative commands:**
- `is_creative: true`
- `composite_name: null` (always, enforced in Python)
- `creative_reasoning` — LLM's explanation of its choices, formatted as:
  `"Axis 1: {choice} × Axis 2: {choice} — [explanation]"`

---

## Other Command Categories

### Recovery Commands
Mapped explicitly in the prompt:
- `"put it back"` / `"undo"` → `return_to_stack()` (safe no-op if not holding)
- `"start over"` / `"never mind"` / `"cancel"` → `clear_assembly()` then `go_home()`
- `"I made a mistake"` → `return_to_stack()` if holding, else `clear_assembly()`
- `"take the X off"` → `return_to_stack()` as best approximation (partial undo not yet supported — see backburner)

### Unknown / Impossible Commands
If an item is not in the system (avocado, pickles, etc.):
- `sequence: []`
- `confidence: 0.1`
- `user_feedback: "I don't have avocado — available ingredients are: bread, meat, cheese, lettuce, tomato"`

The `user_feedback` field is intended to be spoken or displayed to the user. Shown as an amber banner in the observation app.

### Secondary / Learning Commands
Commands that try to define new mappings or name sequences:
- LLM produces the sequence for the underlying action
- Sets `composite_name` to the requested name
- Sets `user_feedback` to `"Mapping/composite noted — will be saved to memory system"`
- The observation app shows a **Save to memory** button on these results
- Clicking it calls `memory_writer.process()` which writes to `instruction_set.json` and `phrase_bank.json`

### Multi-Zone Commands
- Zone names: `assembly_fixture` (default/center), `assembly_left`, `assembly_right`
- Spatial words in commands are mapped: `"left"` → `assembly_left`, `"right"` → `assembly_right`, `"over there"` → `assembly_fixture`
- Pattern: `set_active_zone("zone")` followed by `add_layer()` calls, then `go_home()`, then `set_active_zone()` for next zone

---

## Result Dict Schema

Every call to `interpret()` returns either `None` (empty/trivial input) or a dict with these fields:

| Field | Type | Description |
|-------|------|-------------|
| `interpretation` | str | One sentence: what the LLM understood |
| `sequence` | list | Final list of `{instruction, params}` steps after all passes |
| `pass1_sequence` | list | Snapshot of Pass 1 sequence before validation (for diff display) |
| `composite_name` | str\|null | Snake_case name if this is worth saving; always null for creative |
| `confidence` | float | 0.9–1.0 clear · 0.7–0.9 interpreted · 0.5–0.7 best-guess · <0.5 unclear |
| `validated` | bool | True if Pass 2 ran and found no issues |
| `validation_issues` | list | List of issue strings from Pass 2; tagged `[P3 fixed]` if Pass 3 corrected them |
| `user_feedback` | str\|null | Message to show/speak for impossible commands or learning directives |
| `is_creative` | bool | True if command matched creative patterns |
| `creative_reasoning` | str\|null | LLM's axis choices and explanation (creative commands only) |
| `raw_response` | str\|null | Raw LLM text if JSON parse failed completely |

---

## JSON Parsing Robustness

The LLM occasionally returns malformed JSON. `_parse_json_response()` handles three failure modes in order:

1. **Direct parse** — `json.loads(text.strip())`
2. **Block extraction** — find first `{` and last `}`, try parsing that substring
3. **Newline collapse** — the LLM sometimes writes multiline string values (especially `creative_reasoning`) with literal newlines instead of `\n`, which is invalid JSON. `_collapse_unescaped_newlines()` walks the text character by character, detects when it's inside a string value, and escapes bare newlines before retrying the parse.

If all three fail, `raw_response` is set and the result returns with `confidence: 0.0`.

---

## Confidence and Strict Mode

**Confidence** is LLM-reported, 0.0–1.0:
- `≥ 0.90` — HIGH (green in app)
- `0.75–0.89` — MED (yellow)
- `< 0.75` — LOW (red)

**Strict mode** (toggled in the observation app Home page):
- When on: any result with `confidence < 0.6` on a non-creative command shows a warning banner: `"Strict mode: confidence X.XX — would request confirmation before executing"`
- Creative commands are exempt from strict mode — low confidence on "go wild" is expected and fine
- At full execution time (not just observation): strict mode is the place to insert a confirmation step before dispatching low-confidence sequences to the hardware

**Auto-learning threshold:** composites are only written to `learned_composites` if `confidence ≥ LLM_CONFIDENCE_THRESHOLD` (default 0.80, set in `config.py`). Creative outputs are never learned regardless of confidence.

---

## Cost

Model: `claude-3-haiku-20240307`
Pricing: $0.25/MTok input · $1.25/MTok output

| Scope | API calls | Approx cost |
|-------|-----------|-------------|
| Single standard command | 2 (Pass 1 + Pass 2) | ~$0.001–0.002 |
| Single command with Pass 3 | 3 | ~$0.002–0.003 |
| Full 55-case observation suite | 110–165 | ~$0.07–0.10 |
| 1,000 full suite runs | — | ~$70–100 |

---

## Backburner Items

Documented in `instruction_set.json → _meta.backburner`:

- **Arc/curve motion** — `move_relative` only supports cardinal directions. Future: irregular, smoothed, or arced paths via waypoint interpolation or RAPID `MoveC`. Needs Unity/RobotStudio motion planner support.
- **Multi-broadcast** — broadcasting one instruction to all active zones simultaneously (e.g. "put bread on both stacks") requires a zone-loop execution mode. Current executor processes one zone at a time.
- **Partial undo** — removing a specific item from mid-stack requires the executor to track exact item positions and pick from a non-top position. Current model only supports top-of-stack pick.
- **Feedback channel** — user-facing TTS or text notification for impossible commands. Scaffolded via `user_feedback` field. Full integration with speech output or UI notification pending.
- **Memory pipeline integration** — secondary commands currently write to `learned_composites` and `phrase_bank.json` via the memory writer, but the phrase bank dispatcher does not yet read `learned_aliases`. Loop needs closing.
