# Sequence Interpreter — Design Document

**File:** `SpeechToText/learning/sequence_interpreter.py`
**Model:** `claude-3-haiku-20240307`
**Last updated:** February 2026 (gripper integration, execution queue, cli_control rewrite)

---

## Overview

The sequence interpreter is the LLM layer of the GoFa robot arm voice control pipeline. It sits between raw speech text and executable robot instructions. Its job is to convert a natural language voice command into a structured sequence of named composites that the instruction compiler can execute.

It is a **three-pass LLM pipeline** — generation, validation, and conditional regeneration — with separate handling for creative commands, recovery commands, unknown items, and secondary/learning commands.

---

## Position in the Full Pipeline

```
Voice (Azure ASR)  OR  CLI text (cli_control.py)
  │
  └─▶ Both share the same phrase_bank.json
        └─▶ Phrase bank (exact + fuzzy match — fast path for known phrases)
              │   ↳ modifier-word bypass: commands with modifiers skip
              │     the cache entirely and fall through to the LLM
              └─▶ Sequence Interpreter  ◀── this document
                    └─▶ Instruction Compiler  (composites → primitives)
                          └─▶ InstructionExecutor
                                │  _send_and_wait(): write → poll tcp_ack.json → next step
                                └─▶ tcp_commands.json
                                      └─▶ Unity (TCPHotController.cs)
                                            └─▶ tcp_ack.json
                                                  └─▶ ABB GoFa / RobotStudio

                    └─▶ Memory Writer  (secondary commands only)
                          └─▶ instruction_set.json  (learned_composites)
                          └─▶ phrase_bank.json  (learned_aliases)
```

The sequence interpreter is only reached if the phrase bank fuzzy match does not return a confident hit (or the modifier-word bypass forces a cache skip). It handles everything the phrase bank can't: novel recipes, creative commands, modifiers, multi-zone builds, recovery, and learning directives.

---

## Instruction Tiers (Von Neumann ISA)

The interpreter operates within a three-tier instruction hierarchy defined in `instruction_set.json`:

| Tier | Name | Visibility | Description |
|------|------|-----------|-------------|
| 0 | **Primitives** | LLM-invisible | Atomic hardware calls: `move_to`, `move_relative`, `gripper_set`, `wait`, `set_speed`. The LLM never calls these directly. |
| 1 | **Composites** | LLM-visible | Stored programs built from primitives: `pick_up`, `add_layer`, `go_home`, `set_active_zone`, etc. The LLM generates sequences of these. |
| 2 | **Learned composites** | Names-only in context | Composites generated by the LLM and written to `instruction_set.json` at runtime. The LLM sees their names but not their sequences. |

The LLM is only ever allowed to call composites. The instruction compiler expands each composite into its primitive sequence before execution.

---

## Three-Pass Pipeline

### Pass 1 — Generation

**Purpose:** Convert voice command → JSON sequence
**Temperature:** `0` for standard commands, `1.0` for creative commands
**Max tokens:** `900`

**Input to LLM:**
- Full LLM context from `get_llm_context()`:
  - Available composite instructions with descriptions and parameters
  - Current scene state: items, locations, assembly zone stacks, active zone
  - Speed profiles and distance aliases from `motion_params`
  - Learned composite names (names only, no sequences)
- Structured prompt sections: OUTPUT FORMAT, RULES, CREATIVE COMMANDS, RECOVERY COMMANDS, SPEED+ACTION COMBINED, UNKNOWN/IMPOSSIBLE COMMANDS, SECONDARY/LEARNING COMMANDS, EXAMPLES
- If creative: the full CREATIVE MODE — AXES section with the pre-selected axis combination (see below)
- If Pass 3 retry: a CORRECTION REQUIRED section listing the specific issues to fix

**Output from LLM:**
```json
{
  "interpretation": "one sentence describing what the command means",
  "sequence": [
    {"instruction": "composite_name", "params": {"key": "value"}},
    ...
  ],
  "composite_name": "snake_case_name_or_null",
  "confidence": 0.95,
  "user_feedback": null,
  "creative_reasoning": null
}
```

**What happens if parse fails:** the raw text is preserved in `raw_response`, the result returns with `confidence: 0.0` and an empty sequence. Nothing is silently dropped.

---

### Pass 2 — Validation

**Purpose:** Check Pass 1 output for invalid instruction names, bad params, wrong zone usage
**Temperature:** `0` (always deterministic)
**Max tokens:** `700`
**Runs only if:** Pass 1 produced a non-empty sequence

**Pass 2 skip conditions** (validation is bypassed when):
- Command is creative (`is_creative: true`)
- Pass 1 confidence ≥ 0.88
- Sequence matches a known recipe exactly
- Sequence is empty

**Input to LLM:**
- The voice command and interpretation from Pass 1
- The generated sequence from Pass 1
- Three explicit valid-name lists derived at runtime:
  - `valid_composites` — from `instruction_compiler.get_composites().keys()`
  - `valid_items` — from `instruction_compiler.get_items().keys()`
  - `valid_locations` — from `instruction_compiler.get_locations().keys()`

**Validation rules enforced:**
1. Every `instruction` value must be in `valid_composites` — fix or remove if not (e.g. `move_absolute` → remove)
2. Every `item` param must be in `valid_items` — remove steps with unknown items
3. Every `location` param must be in `valid_locations`
4. `move_relative` direction must be one of: `right`, `left`, `up`, `down`, `forward`, `backward`
5. Assembly sequences must use `add_layer`, not `transfer` or `place_at`
6. `set_active_zone` zone must be one of: `assembly_fixture`, `assembly_left`, `assembly_right`

**Post-validation safety net (Python-level):**
After Pass 2, Python applies a final cleanup pass regardless of what the LLM returned:
- `place_at` and `transfer` instructions are converted to `add_layer`
- Steps with items not in `valid_items` are removed
- If the sequence ends with `add_layer` and no `go_home`, `go_home()` is auto-appended

**Output from LLM:**
```json
{
  "valid": true,
  "issues": ["list of what was wrong — empty if clean"],
  "sequence": [corrected sequence]
}
```

**Result fields set after Pass 2:**
- `sequence` → replaced with corrected version
- `validated` → `true` if no issues, `false` if issues found or validator response unparseable
- `validation_issues` → list of issue strings, shown in the app bubble
- `pass1_sequence` → snapshot of the original Pass 1 sequence (preserved for side-by-side display)

---

### Pass 3 — Regeneration

**Purpose:** If Pass 2 found real structural issues, feed them back to the generator for one correction retry
**Temperature:** `0` (correction passes are always deterministic)
**Max tokens:** `900`
**Runs only if:** Pass 2 found issues that are not "unparseable" or "API error"

**What gets injected into the prompt:**
A `CORRECTION REQUIRED (Pass 3)` section is prepended to the normal prompt:
```
━━━ CORRECTION REQUIRED (Pass 3) ━━━
Your previous response had these problems that the validator caught:
  - move_absolute is not a valid instruction
  - transfer used for assembly zone — should be add_layer

Please regenerate a corrected sequence that fixes ALL of these issues.
Use ONLY valid instruction names from the list above.
```

**Result:** if Pass 3 produces a valid response, its sequence replaces the Pass 2 output. Validation issues are tagged `[P3 fixed]`. If Pass 3 fails, the Pass 2 result is kept — Pass 3 is non-fatal.

---

## Creative Command Handling

### Detection

Creative commands are detected in Python **before** any LLM call, using a regex pattern list:

```python
_CREATIVE_PATTERNS = [
    r"\bimpress\b", r"\bgo wild\b", r"\bsurprise\b", r"\bcreative\b",
    r"\bbeautiful\b", r"\bwork of art\b", r"\bbuild a tower\b",
    r"\bbest .* can\b", r"\bsomething delicious\b", r"\bdo something\b",
    r"\bgo crazy\b", r"\bhave fun\b", r"\bfancy\b", r"\belaborate\b",
    ...
]
```

If `_is_creative(command)` returns `True`:
- Pass 1 temperature is set to `1.0` (probabilistic sampling, different output every run)
- `composite_name` is force-nulled in Python regardless of what the LLM returns (creative outputs are never saved as composites)
- The CREATIVE MODE — AXES section is injected into the prompt

### Axis System

The creative section uses a **two-axis composition model** instead of named archetypes. Python pre-selects one option from each axis using `random.choice()` and injects it as a concrete assignment before the LLM sees the prompt. The LLM cannot pick its own favorite — it receives a specific combination and must execute it.

**Axis 1 — Spatial Structure** (how layers are distributed across zones and height):

| Option | Instruction |
|--------|------------|
| `single_tall` | One zone only — stack as many layers as possible |
| `single_short` | One zone only — exactly 2–3 layers, stop there |
| `three_zone_split` | Use all three zones via `set_active_zone()` — three builds in sequence |
| `two_zone_contrast` | Two zones with opposing ingredient sets |
| `palindrome` | One zone — sequence reads identically forwards and backwards |
| `inverted` | One zone — reverse of conventional order, bread absolutely last |

**Axis 2 — Ingredient Logic** (which ingredients are used and why):

| Option | Instruction |
|--------|------------|
| `all_one` | One ingredient repeated 4–6 times, nothing else |
| `category_grouped` | All proteins first, then all vegetables, then starch/dairy |
| `full_set` | Every available ingredient at least once, unusual order |
| `random_ordered` | All ingredients in a non-obvious, non-conventional order |
| `doubled` | Exactly two ingredients alternating: A,B,A,B,A,B |
| `constrained` | Only ingredients explicitly named in the command (auto-selected when command names items) |

**Compatibility guards:** some axis combinations are logically contradictory and blocked:

```python
_INCOMPATIBLE_PAIRS = {
    "palindrome": {"random_ordered", "category_grouped", "full_set"},
    "inverted":   {"all_one"},
}
```

If the random draw produces an incompatible pair, Python retries up to 10 times until a valid pair is found.

**Constrained override:** if the voice command names specific ingredients (e.g. "do something with the lettuce and tomato"), Axis 2 is automatically set to `constrained` with those ingredients listed explicitly. Axis 1 is still random.

**What gets injected into the prompt for creative commands:**
```
YOUR ASSIGNED COMBINATION FOR THIS COMMAND:

  AXIS 1 (spatial structure): palindrome
  → one zone — the ingredient sequence must read IDENTICALLY forwards and backwards...

  AXIS 2 (ingredient logic): doubled
  → pick exactly TWO ingredients and alternate them: A,B,A,B,A,B. Nothing else.

You MUST follow both axes literally when building the sequence.
...
- palindrome: write out your planned sequence first in creative_reasoning, verify it
  reads the same backwards, THEN generate the JSON steps.
- inverted: the LAST step before go_home() must be add_layer(bread). Bread goes last.
```

**Result fields for creative commands:**
- `is_creative: true`
- `composite_name: null` (always, enforced in Python)
- `creative_reasoning` — LLM's explanation of its choices, formatted as:
  `"Axis 1: {choice} × Axis 2: {choice} — [explanation]"`

---

## Other Command Categories

### Recovery Commands
Mapped explicitly in the prompt:
- `"put it back"` / `"undo"` → `return_to_stack()` (safe no-op if not holding)
- `"start over"` / `"never mind"` / `"cancel"` → `clear_assembly()` then `go_home()`
- `"I made a mistake"` → `return_to_stack()` if holding, else `clear_assembly()`
- `"take the X off"` → `return_to_stack()` as best approximation (partial undo not yet supported — see backburner)

### Unknown / Impossible Commands
If an item is not in the system (avocado, pickles, etc.):
- LLM may substitute the closest available ingredient (e.g. avocado → tomato as "closest juicy topping") or return an empty sequence with low confidence
- `confidence: 0.1–0.8` depending on how reasonable the substitution is
- `user_feedback` describes what was done: `"No avocado — picking up tomato as closest match"`

The `user_feedback` field is intended to be spoken or displayed to the user. Shown as an amber banner in the observation app.

### Secondary / Learning Commands
Commands that try to define new mappings or name sequences:
- LLM produces the sequence for the underlying action
- Sets `composite_name` to the requested name
- Sets `user_feedback` to `"Mapping/composite noted — will be saved to memory system"`
- The observation app shows a **Save to memory** button on these results
- Clicking it calls `memory_writer.process()` which writes to `instruction_set.json` and `phrase_bank.json`

### Multi-Zone Commands
- Zone names: `assembly_fixture` (default/center), `assembly_left`, `assembly_right`
- Spatial words in commands are mapped: `"left"` → `assembly_left`, `"right"` → `assembly_right`, `"over there"` → `assembly_fixture`
- Pattern: `set_active_zone("zone")` followed by `add_layer()` calls, then `go_home()`, then `set_active_zone()` for next zone

---

## Phrase Bank Cache — How the Fast Path Works

Before the LLM is called at all, the phrase bank attempts to serve the command from its cache of previously learned sequences. There are two lookup types:

1. **Exact match** — `phrase_bank.sequence_match(phrase)` — literal string lookup, O(1)
2. **Fuzzy match** — `phrase_bank.fuzzy_sequence_match(phrase)` — `SequenceMatcher.ratio()` over all cached keys, returns the best hit above `FUZZY_MATCH_THRESHOLD`

If either hits, the cached result dict is returned immediately with `_from_cache: true` and no LLM call is made. This is how "make a BLT" returns in ~0ms on repeat.

### Modifier-Word Bypass (Guard 1)

A command that contains modifier words carries intent that a plain cached recipe cannot satisfy. `fuzzy_sequence_match` checks for these tokens **before** doing any fuzzy scoring, and returns `None` immediately if any are found:

| Category | Tokens |
|----------|--------|
| Omissions | `no`, `without`, `hold`, `remove`, `skip` |
| Additions / multipliers | `extra`, `double`, `triple`, `more` |
| Substitutions | `swap`, `replace`, `instead`, `sub` |
| Speed modifiers | `slow`, `slowly`, `fast`, `faster`, `quickly` |
| Adverbial modifiers | `nice and`, `careful`, `gently` |
| Spatial deictics | `over there`, `right there`, `over here` |
| `with X` (ingredient append) | `with` — but **not** `with a` (preserved for "with a side of…" style) |

Examples of what this fixes:
- `"make a classic with no tomato"` → bypasses cache → LLM returns 4-layer sequence (no tomato) ✓
- `"make a club sandwich with extra meat"` → bypasses cache → LLM returns 8-layer sequence (double meat) ✓
- `"make a BLT nice and slow"` → bypasses cache → LLM prepends `adjust_speed(slow)` ✓
- `"make a BLT"` → cache hit as normal → ~0ms ✓

### Pick-Up Item-Noun Guard (Guard 2)

For pick-up style phrases (`pick up the X`, `grab the X`, `carefully pick up the X`), the terminal item word must match exactly — a fuzzy hit on a different item is rejected. This prevents `"pick up the avocado"` from hitting `"carefully pick up the lettuce"` at 0.6 similarity.

The regex covers adverb-prefixed variants:
```
^(?:(?:carefully?|gently?|slowly?|quickly?)\s+)?
 (?:pick\s+up|grab|get|take)\s+(?:the\s+)?(\w+)$
```

---

## Execution Queue — From Sequence to Unity

After the sequence interpreter and compiler expand composites to primitives, `InstructionExecutor` drives each primitive to Unity one step at a time.

### Move primitives — `_send_and_wait()`

```
1. Write {x, y, z, gripper_position} → tcp_commands.json
2. Record write_time = time.time()
3. Poll tcp_ack.json every 50ms
4. Accept ack only if os.path.getmtime(tcp_ack.json) >= write_time  (stale-ack detection)
5. Read back confirmed {x, y, z} from ack → update local position state
6. Proceed to next primitive
   (timeout: 10s → warn and use local position)
```

Unity writes `tcp_ack.json` via `TCPHotController.cs → WriteAcknowledgment()` after `"TCP reached target"`. The mtime comparison (rather than an ISO timestamp string) is used because Unity writes timestamps at second resolution, while `write_time` is a sub-second float — a same-second ack would otherwise appear stale.

### Gripper primitives — fire-and-forget

`gripper_open` and `gripper_close` update `self.gripper_position`, write position+gripper to `tcp_commands.json`, and return immediately without waiting for ack. Unity picks up the gripper change on the next file read. No `tcp_ack.json` is issued for gripper-only changes.

### Startup sync

On init, `InstructionExecutor` reads the current `tcp_commands.json` to initialize `self.gripper_position` and current position. This prevents the executor from drifting out of sync with Unity across restarts.

### tcp_ack.json schema

Written by `TCPHotController.cs` after each move:
```json
{
  "completed": true,
  "position": {
    "x": 0.123,
    "y": 0.567,
    "z": -0.238,
    "gripper_position": 0.11
  },
  "timestamp": "2026-02-22T14:35:01.123456+00:00"
}
```

---

## Result Dict Schema

Every call to `interpret()` returns either `None` (empty/trivial input) or a dict with these fields:

| Field | Type | Description |
|-------|------|-------------|
| `interpretation` | str | One sentence: what the LLM understood |
| `sequence` | list | Final list of `{instruction, params}` steps after all passes |
| `pass1_sequence` | list | Snapshot of Pass 1 sequence before validation (for diff display) |
| `composite_name` | str\|null | Snake_case name if this is worth saving; always null for creative |
| `confidence` | float | 0.9–1.0 clear · 0.7–0.9 interpreted · 0.5–0.7 best-guess · <0.5 unclear |
| `validated` | bool | True if Pass 2 ran and found no issues |
| `validation_issues` | list | List of issue strings from Pass 2; tagged `[P3 fixed]` if Pass 3 corrected them |
| `user_feedback` | str\|null | Message to show/speak for impossible commands or learning directives |
| `is_creative` | bool | True if command matched creative patterns |
| `creative_reasoning` | str\|null | LLM's axis choices and explanation (creative commands only) |
| `raw_response` | str\|null | Raw LLM text if JSON parse failed completely |
| `_from_cache` | bool | True if result was served from phrase bank (no LLM call made) |

---

## JSON Parsing Robustness

The LLM occasionally returns malformed JSON. `_parse_json_response()` handles three failure modes in order:

1. **Direct parse** — `json.loads(text.strip())`
2. **Block extraction** — find first `{` and last `}`, try parsing that substring
3. **Newline collapse** — the LLM sometimes writes multiline string values (especially `creative_reasoning`) with literal newlines instead of `\n`, which is invalid JSON. `_collapse_unescaped_newlines()` walks the text character by character, detects when it's inside a string value, and escapes bare newlines before retrying the parse.

If all three fail, `raw_response` is set and the result returns with `confidence: 0.0`.

---

## Confidence and Strict Mode

**Confidence** is LLM-reported, 0.0–1.0:
- `≥ 0.90` — HIGH (green in app)
- `0.75–0.89` — MED (yellow)
- `< 0.75` — LOW (red)

**Strict mode** (toggled in the observation app Home page):
- When on: any result with `confidence < 0.6` on a non-creative command shows a warning banner: `"Strict mode: confidence X.XX — would request confirmation before executing"`
- Creative commands are exempt from strict mode — low confidence on "go wild" is expected and fine
- At full execution time (not just observation): strict mode is the place to insert a confirmation step before dispatching low-confidence sequences to the hardware

**Auto-learning threshold:** composites are only written to `learned_composites` if `confidence ≥ LLM_CONFIDENCE_THRESHOLD` (default 0.80, set in `config.py`). Creative outputs are never learned regardless of confidence.

---

## Cost

Model: `claude-3-haiku-20240307`
Pricing: $0.25/MTok input · $1.25/MTok output

| Scope | API calls | Approx cost |
|-------|-----------|-------------|
| Single standard command | 2 (Pass 1 + Pass 2) | ~$0.001–0.002 |
| Single command with Pass 3 | 3 | ~$0.002–0.003 |
| Full 55-case observation suite | 110–165 | ~$0.07–0.10 |
| 1,000 full suite runs | — | ~$70–100 |

Cache hits (`_from_cache: true`) cost $0.

---

## Known Issues and Tuning Notes

These are areas that currently work but have sharp edges worth watching as the system grows or before integration.

### Fuzzy Match Threshold (0.6) is Aggressive

`FUZZY_MATCH_THRESHOLD = 0.6` in `config.py` is intentionally permissive to catch phrasing variants. As the phrase bank grows with more cached entries, the risk of false positives increases — two different commands can score above 0.6 against each other even if they mean different things.

**Symptoms of threshold being too low:**
- A modifier command (e.g. `"make a classic with no tomato"`) getting served the wrong base-recipe result from cache
- A command for one recipe hitting the cache entry for a different recipe

**Current mitigation:** the modifier-word bypass in `fuzzy_sequence_match` intercepts most of these cases before the ratio is even computed. But if new command categories are added that don't contain any bypass tokens, this can resurface.

**Recommendation:** if the phrase bank grows beyond ~50 sequence entries, consider raising `FUZZY_MATCH_THRESHOLD` from `0.6` to `0.75` via the env var and re-running the observation suite to check for regressions. The threshold is intentionally configurable via `.env`:
```env
FUZZY_MATCH_THRESHOLD=0.75
```

### Pick-Up Item Noun Guard Only Covers `pick up / grab / get / take`

The item-noun guard in `fuzzy_sequence_match` (Guard 2) uses a regex that covers the most common pick-up verbs with optional adverb prefixes. If new pick-up style phrases are added to the phrase bank using different verbs (e.g. `"fetch the tomato"`, `"retrieve the cheese"`), those would not be protected and could fuzzy-hit entries with different items.

**Fix if needed:** extend the `_PICKUP_RE` regex in `fuzzy_sequence_match` to include additional verb forms.

### `"make two sandwiches"` Hits Cache as One Sandwich

`"make two sandwiches"` fuzzy-matches `"make a sandwich"` at ratio 0.82, which exceeds the 0.6 threshold and contains no modifier-bypass tokens, so the cache serves a single classic sandwich. The multi-sandwich intent is silently dropped.

**Root cause:** `"two"` / `"a"` word swap doesn't trigger the bypass, and the system has no quantity-awareness in the cache layer.

**Workaround:** add `"make two sandwiches"` as a separate named phrase bank entry, or add `\btwo\b|\bthree\b|\b\d+\b` to the modifier-bypass pattern if quantity words should always go to the LLM.

### Creative Mode Bypass of Pass 2

Creative commands skip Pass 2 validation entirely (`is_creative` short-circuits validation). This means the LLM's creative output is never checked for invalid instruction names or params — it goes straight to the post-validation Python safety net, which only catches `place_at`/`transfer` rewrites and unknown items. Other invalid instructions in creative output would pass through unchecked.

**In practice:** Haiku rarely produces invalid creative sequences. But if a creative command consistently produces executor errors, enable verbose logging and check `pass1_sequence` vs the final `sequence`.

### Gripper Changes Are Fire-and-Forget (No Ack)

Gripper open/close primitives do not call `_send_and_wait()` — they write to `tcp_commands.json` and return immediately. Unity will pick up the gripper state on the next file read, but `tcp_ack.json` is only written after a move completes. Consequence: if a sequence does `gripper_close` immediately followed by `move_to`, the move ack confirms the move position but does not confirm the gripper width. In practice this works reliably (commands are processed in order), but there is no readback for gripper state.

### Ack Timeout Is Conservative for Simulation

`ACK_TIMEOUT = 10.0` seconds was set to accommodate real EGM/RobotStudio latency. In Unity-only simulation, moves typically complete in 0.5–2 seconds. If step throughput feels slow in simulation, this constant can safely be lowered to 2–3s. Keep it higher when real hardware is in the loop.

### Stale Ack Detection Uses File Mtime (Not Timestamp String)

The ack-wait uses `os.path.getmtime(tcp_ack.json) >= write_time` rather than parsing the ISO timestamp inside the file. This is intentional: Unity writes timestamps at second resolution, while Python's `write_time` is a sub-second float. A same-second ack would compare as stale if timestamp strings were used directly. Mtime is OS-level and reliable. Note: on some network filesystems (NFS, CIFS), mtime granularity can be coarser — if deploying on such a filesystem, consider an alternative mechanism.

### Phrase Bank Growth and `usage_count` Drift

Every cache hit increments `usage_count` in `phrase_bank.json` and triggers an immediate file save. In high-throughput scenarios (e.g. running the 55-case observation suite repeatedly), this results in many small disk writes per run. The phrase bank file is also getting progressively larger as `usage_count` values grow.

**No functional impact** currently, but worth noting for integration: if the system is being run in a tight loop for automated testing, consider initializing `PhraseBank(auto_save=False)` to suppress mid-session saves.

### `"start a BLT over there"` — Zone Resolution

`"over there"` is listed in the modifier-bypass token list (`over\s+there\b`), so this command now correctly bypasses the cache and reaches the LLM. The LLM currently resolves it to `assembly_fixture` (the default/center zone) since no specific zone can be inferred from `"over there"` alone. In a live integration context with a physical pointing gesture or gaze direction, this would need to be resolved differently.

---

## Backburner Items

Documented in `instruction_set.json → _meta.backburner`:

- **Arc/curve motion** — `move_relative` only supports cardinal directions. Future: irregular, smoothed, or arced paths via waypoint interpolation or RAPID `MoveC`. Needs Unity/RobotStudio motion planner support.
- **Multi-broadcast** — broadcasting one instruction to all active zones simultaneously (e.g. "put bread on both stacks") requires a zone-loop execution mode. Current executor processes one zone at a time.
- **Partial undo** — removing a specific item from mid-stack requires the executor to track exact item positions and pick from a non-top position. Current model only supports top-of-stack pick.
- **Feedback channel** — user-facing TTS or text notification for impossible commands. Scaffolded via `user_feedback` field. Full integration with speech output or UI notification pending.
- **Memory pipeline integration** — secondary commands currently write to `learned_composites` and `phrase_bank.json` via the memory writer, but the phrase bank dispatcher does not yet read `learned_aliases`. Loop needs closing.
- **Quantity awareness** — commands like "make two sandwiches" or "add three layers of cheese" have no quantity-dispatch mechanism. Currently falls through to a single-instance cache hit or LLM interpretation without repetition logic.
- **Gripper ack confirmation** — gripper open/close changes are currently fire-and-forget (no `tcp_ack.json` readback). A future improvement would have Unity write a gripper-specific ack (or include gripper confirmation in the move ack) to allow the executor to verify the gripper reached its target width before proceeding.
- **RobotStudio / EGM integration** — the current `tcp_commands.json` file-based IPC works for Unity simulation. Full integration with an ABB EGM socket connection or RAPID program for live hardware is pending.
